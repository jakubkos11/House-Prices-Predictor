{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nimport optuna\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, FunctionTransformer, StandardScaler\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score, make_scorer\n\nclass Config:\n    \"\"\"A class that stores all configurations and constants.\"\"\"\n    # Main toggler to decide whether we want to use optuna tuning or use default parameters\n    TUNE_HYPERPARAMETERS = False\n\n    # Number of trials in Optuna\n    N_TRIALS_OPTUNA = 50\n    \n    # Optimal number of clusters in K-Means algorithm\n    OPTIMAL_K_CLUSTERS = 12\n    \n    # Best xgboost parameters found previously by optuna\n    BEST_PARAMS = {\n        'n_estimators': 2353,\n        'learning_rate': 0.015,\n        'max_depth': 3, \n        'subsample': 0.719,\n        'colsample_bytree': 0.678,\n        'reg_alpha': 0.016,\n        'reg_lambda': 3.652,\n        'min_child_weight': 5.172\n    }\n    \n    # Mapping ordinal features\n    ORDINAL_FEATURE_MAP = {\n        'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'BsmtQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'BsmtCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'BsmtExposure': ['None', 'No', 'Mn', 'Av', 'Gd'],\n        'BsmtFinType1': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n        'BsmtFinType2': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n        'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'GarageFinish': ['None', 'Unf', 'RFn', 'Fin'],\n        'GarageQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'GarageCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'PavedDrive': ['N', 'P', 'Y'],\n        'Fence': ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],\n        'OverallQual': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        'OverallCond': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n        'FireplaceQu': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n        'PoolQC': ['None', 'Fa', 'TA', 'Gd', 'Ex'],\n        'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],\n        'LandContour': ['Low', 'HLS', 'Bnk', 'Lvl'],\n        'LandSlope': ['Sev', 'Mod', 'Gtl']\n    }\n\n    # Lists of features grouped by different types\n    \n    NOMINAL_FEATURES = [\n        'MSSubClass', 'MSZoning', 'Alley', 'LotConfig', 'Neighborhood',\n        'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n        'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n        'Heating', 'Electrical', 'GarageType', 'MiscFeature', 'SaleType',\n        'SaleCondition', 'House_Cluster'\n    ]\n\n    SKEWED_FEATURES = [\n        'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n        'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n        '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n        'HalfBath', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n        'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n        'PoolArea', 'MiscVal', 'GarageArea', 'AgeHouse', 'AgeSinceRemod',\n        'TotalSF', 'TotalPorchSF', 'GarageAge'\n    ]\n    \n    NUMERIC_FEATURES = [\n        'FullBath', 'BedroomAbvGr', 'GarageCars', 'Month_sin', 'Month_cos', \n        'YrSold', 'CentralAir', 'IsRemodeled', 'HasPorch', 'HasGarage', \n        'Has2ndFlr', 'HasBsmt', 'HasFireplace', 'HasPool', 'IsNewHouse',\n        'TotalBaths', 'GarageAreaPerCar', 'OverallScore', 'OverallQual_sq', \n        'BsmtFinRatio', 'BedBathRatio'\n    ]\n\nclass HousingPricePredictor:\n    \"\"\"Class used for training a model and predicting house prices.\"\"\"\n    def __init__(self, config):\n        self.config = config\n        self.preprocessor = None\n        self.model = None\n\n    @staticmethod\n    def _print_evaluation_metrics(y_true, y_pred, dataset_name):\n        \"\"\"Print evaluation metrics for given dataset.\"\"\"\n        print(f\"\\n=== {dataset_name} metrics ===\")\n        print(f\"RÂ²: {r2_score(np.expm1(y_true), np.expm1(y_pred)):.4f}\")\n        print(f\"RMSLE: {mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred), squared = False):.4f}\")\n        print(f\"MAE: {mean_absolute_error(np.expm1(y_true), np.expm1(y_pred)):.2f}\")\n        \n    def _clean_data(self, data):\n        \"\"\"Handles missing values correctly based on their meaning.\"\"\"\n        df = data.copy()\n\n        # Define strategies of handling with missing data\n        cols_na_as_none = [\n            'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n            'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish',\n            'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'MasVnrType'\n        ]\n        cols_na_as_zero = [\n            'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n            'BsmtHalfBath', 'GarageArea', 'GarageCars', 'MasVnrArea'\n        ]\n\n        # Columns to fill with the most frequent value per category\n        cols_na_as_mode = [\n            'MSZoning', 'Functional', 'Electrical',\n            'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType'\n        ]\n\n        # Fill missed values in specified columns\n        for col in cols_na_as_none:\n            df[col] = df[col].fillna('None')\n\n        for col in cols_na_as_zero:\n            df[col] = df[col].fillna(0)\n\n        for col in cols_na_as_mode:\n            df[col] = df[col].fillna(df[col].mode()[0])\n\n        # Fill with median from neighborhood, then with global median if there are empty values left\n        df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(\n            lambda x: x.fillna(x.median()))\n        \n        if df['LotFrontage'].isnull().any():\n            df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].median())\n\n        # If no info about GarageYrBlt then fill with house year built\n        df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt'])\n\n        # Switch formats of some variables for convinience\n        df['MSSubClass'] = df['MSSubClass'].astype(str)\n        df['CentralAir'] = df['CentralAir'].map({'Y': 1, 'N': 0})\n\n        return df\n        \n    def _apply_feature_engineering(self, data):\n        \"\"\"Add custom engineering features to improve predictive power of the model.\"\"\"\n        df = data.copy()\n        # Features based on time\n        df['AgeHouse'] = (df['YrSold'] - df['YearBuilt']).clip(lower = 0)\n        df['AgeSinceRemod'] = (df['YrSold'] - df['YearRemodAdd']).clip(lower = 0)\n        df['IsRemodeled'] = (df['YearRemodAdd'] > df['YearBuilt']).astype(int)\n        df['IsNewHouse'] = (df['YearBuilt'] == df['YrSold']).astype(int)\n        df['GarageAge'] = (df['YrSold'] - df['GarageYrBlt']).clip(lower = 0)\n    \n        # Features based on total surface\n        df['TotalSF'] = df[['TotalBsmtSF','1stFlrSF','2ndFlrSF']].sum(axis = 1).clip(lower = 0)\n        df['TotalPorchSF'] = df[['OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','WoodDeckSF']].sum(axis = 1)\n\n        # Binary features\n        df['HasPorch'] = (df['TotalPorchSF'] > 0).astype(int)\n        df['Has2ndFlr'] = (df['2ndFlrSF'] > 0).astype(int)\n        df['HasBsmt'] = (df['TotalBsmtSF'] > 0).astype(int)\n        df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)\n        df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n        df['HasGarage'] = (df['GarageArea'] > 0).astype(int)\n\n        # Features based on number of rooms\n        df['TotalBaths'] = df['FullBath'] + 0.5 * df['HalfBath'] + df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n\n        # Proportion indicators\n        df['GarageAreaPerCar'] = df['GarageArea'] / df['GarageCars'].replace(0,1)\n        df['BsmtFinRatio'] = df['BsmtFinSF1'] / df['TotalBsmtSF'].replace(0, 1)\n        df['BedBathRatio'] = df['BedroomAbvGr'] / df['TotalBaths'].replace(0,1)\n\n        # Interaction and quality features\n        df['OverallScore'] = df['OverallQual'] * df['OverallCond']\n        df['QualPerSF'] = df['OverallQual'] / df['GrLivArea'].replace(0,1)\n        df[\"OverallQual_sq\"] = df[\"OverallQual\"] ** 2\n        df[\"GrLivArea_sq\"] = np.log1p(df[\"GrLivArea\"])\n\n        # Seasonal features\n        df['Month_sin'] = np.sin(2 * np.pi * df['MoSold'] / 12)\n        df['Month_cos'] = np.cos(2 * np.pi * df['MoSold'] / 12)\n\n        # Drop unnecessary and noise data\n        df = df.drop(['MoSold', 'Utilities', 'Street'], axis = 1, errors = 'ignore')\n\n        return df\n\n    def _filter_outliers(self, X, y):\n        \"\"\"Detect and remove outliers from training set.\"\"\"\n        # Select only skewed features with high feature importance for selection\n        outlier_detection_features = ['GrLivArea', 'OverallQual', 'TotalSF', \n                                      'YearBuilt', 'TotalBaths', 'GarageArea']\n        \n        clf = IsolationForest(n_estimators = 100, contamination = 0.005, random_state = 1)\n        outlier_preds = clf.fit_predict(X[outlier_detection_features].fillna(0).values) # Return 1 for normal and -1 for outliers\n        \n        non_outlier_indices = np.where(outlier_preds == 1)[0]\n        \n        return X.iloc[non_outlier_indices], y.iloc[non_outlier_indices]\n\n    def _add_cluster_features(self, X, X_test):\n        \"\"\"Adds cluster features for K-Means algorithm.\"\"\"\n        cluster_features = ['OverallQual', 'GrLivArea', 'TotalSF', 'AgeHouse', 'TotalBaths', 'OverallScore']\n        \n        # Scaling is required for K-Means algorithm\n        scaler = StandardScaler()\n        kmeans = KMeans(n_clusters = self.config.OPTIMAL_K_CLUSTERS, random_state = 1, n_init = 'auto')\n\n        # Adjust scaler only to training data\n        scaled_train_features = scaler.fit_transform(X[cluster_features].fillna(0))\n        kmeans.fit(scaled_train_features)\n        \n        # Use objects for transformation of both datasets\n        X['House_Cluster'] = kmeans.predict(scaled_train_features)\n        scaled_test_features = scaler.transform(X_test[cluster_features].fillna(0))\n        X_test['House_Cluster'] = kmeans.predict(scaled_test_features)\n        \n        return X, X_test\n    \n    def _build_preprocessor(self):\n        \"\"\"Build pipeline for data preprocessing.\"\"\"\n        # Transform ordinal map into keys and values\n        ordinal_features = list(self.config.ORDINAL_FEATURE_MAP.keys())\n        ordinal_categories = list(self.config.ORDINAL_FEATURE_MAP.values())\n        \n        # Pipeline for numerical features, fill missing values with median\n        numeric_transformer = Pipeline(steps = [\n            ('imputer', SimpleImputer(strategy = 'median'))\n        ])\n    \n        # Pipeline for skewed numerical features\n        skewed_transformer = Pipeline(steps = [\n            ('imputer', SimpleImputer(strategy = 'median')),\n            ('log', FunctionTransformer(np.log1p, validate = False)),\n            ('scaler', StandardScaler())\n        ])\n\n        # Pipeline for ordinal features\n        ordinal_transformer = Pipeline(steps = [\n            ('imputer', SimpleImputer(strategy = 'most_frequent')),\n            ('encoder', OrdinalEncoder(\n                categories = ordinal_categories,\n                handle_unknown = 'use_encoded_value',\n                unknown_value = -1\n            ))\n        ])\n    \n        # Pipeline for nominal features\n        nominal_transformer = Pipeline(steps = [\n            ('imputer', SimpleImputer(strategy = 'most_frequent')),\n            ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse_output = False))\n        ])\n    \n        # Combine all pipelines in single ColumnTransformer\n        preprocessor = ColumnTransformer(transformers = [\n            ('num', numeric_transformer, self.config.NUMERIC_FEATURES),\n            ('skewed', skewed_transformer, self.config.SKEWED_FEATURES),\n            ('ord', ordinal_transformer, ordinal_features),\n            ('nom', nominal_transformer, self.config.NOMINAL_FEATURES)\n        ], remainder = 'passthrough')\n        return preprocessor\n\n    def tune_hyperparameters(self, X, y):\n        \"\"\"Runs Optuna to find optimal hyperparameters.\"\"\"\n        def objective(trial):\n            # Search through different hyperparameters\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n                'max_depth': trial.suggest_int('max_depth', 3, 7),\n                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n                'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n                'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n                'min_child_weight': trial.suggest_float('min_child_weight', 1, 10)\n            }\n            model = XGBRegressor(**params, n_jobs = -1, random_state = 1)\n            pipeline = Pipeline(steps = [\n                ('preprocessor', self.preprocessor),\n                ('regressor', model)\n            ])\n            kfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n            # Return positive RMSLE, because Optuna has to minimize it\n            rmsle_scorer = make_scorer(lambda y_true_log, y_pred_log: \n                                       mean_squared_log_error(\n                                            np.expm1(y_true_log), \n                                            np.expm1(y_pred_log),\n                                            squared = False  \n                                        ), greater_is_better = False)\n            score = -cross_val_score(pipeline, X, y, cv = kfold, scoring = rmsle_scorer).mean()\n\n            return score\n\n        # Optimization loop\n        study = optuna.create_study(direction = 'minimize')\n        study.optimize(objective, n_trials = self.config.N_TRIALS_OPTUNA, show_progress_bar = True)\n        return study.best_params\n\n    def train(self, X, y):\n        \"\"\"Train final model with best hyperparameters.\"\"\"\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\n        # Build preprocessor based on final set of features\n        self.preprocessor = self._build_preprocessor()\n\n        params = self.config.BEST_PARAMS\n        if self.config.TUNE_HYPERPARAMETERS:\n            params = self.tune_hyperparameters(X_train, y_train)\n\n        self.model = XGBRegressor(**params, random_state = 1, n_jobs = -1)\n\n        # Build final pipeline\n        pipeline = Pipeline(steps = [\n            ('preprocessor', self.preprocessor),\n            ('regressor', self.model)\n        ])\n        pipeline.fit(X_train, y_train)\n\n        # Evaluation on train and validation sets\n        y_pred_train = pipeline.predict(X_train)\n        y_pred_val = pipeline.predict(X_val)  \n\n        # Show performance metrics\n        print('Best parameters: \\n', params)\n        self._print_evaluation_metrics(y_train, y_pred_train, \"Training\")\n        self._print_evaluation_metrics(y_val, y_pred_val, \"Validation\")\n\n        # Train final model\n        pipeline.fit(X, y)\n        self.model = pipeline\n\n    def predict(self, X_test):\n        \"\"\"Generate predictions on test data.\"\"\"\n        return self.model.predict(X_test)\n\n    def run(self):\n        \"\"\"Runs whole pipeline.\"\"\"\n        # Load both datasets\n        train_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n        test_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n        test_ids = test_df['Id']\n        \n        # Separate features and target\n        X = train_df.drop(['SalePrice', 'Id'], axis = 1, errors = 'ignore')\n        y = np.log1p(train_df['SalePrice'])\n        X_test = test_df.drop(['Id'], axis = 1, errors = 'ignore')\n        \n        # Clean the data\n        X = self._clean_data(X)\n        X_test = self._clean_data(X_test)\n    \n        # Add new features to the model\n        X = self._apply_feature_engineering(X)\n        X_test = self._apply_feature_engineering(X_test)\n\n        # Apply new feature based on grouping houses on different types\n        X, X_test = self._add_cluster_features(X, X_test)\n\n        # Remove houses with extremaly skewed features\n        X, y = self._filter_outliers(X, y)\n\n        # Reset indices after removing unwanted features\n        X = X.reset_index(drop = True)\n        y = y.reset_index(drop = True)\n        X_test = X_test.reset_index(drop = True)\n    \n        # Train model\n        self.train(X, y)\n    \n        # Predictions\n        final_predictions = self.predict(X_test)\n    \n        # Generate submission file\n        submission = pd.DataFrame({'Id': test_ids, 'SalePrice': np.expm1(final_predictions)})\n        submission.to_csv('submission.csv', index = False)\n\n# Main function\nif __name__ == '__main__':\n    config = Config()\n    predictor = HousingPricePredictor(config)\n    predictor.run()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T21:35:17.295595Z","iopub.execute_input":"2025-09-20T21:35:17.296099Z","iopub.status.idle":"2025-09-20T21:35:23.653673Z","shell.execute_reply.started":"2025-09-20T21:35:17.296064Z","shell.execute_reply":"2025-09-20T21:35:23.652730Z"}},"outputs":[{"name":"stdout","text":"Best parameters: \n {'n_estimators': 2353, 'learning_rate': 0.015, 'max_depth': 3, 'subsample': 0.719, 'colsample_bytree': 0.678, 'reg_alpha': 0.016, 'reg_lambda': 3.652, 'min_child_weight': 5.172}\n\n=== Training metrics ===\nRÂ²: 0.9861\nRMSLE: 0.0483\nMAE: 6167.04\n\n=== Validation metrics ===\nRÂ²: 0.9231\nRMSLE: 0.1209\nMAE: 13139.42\n","output_type":"stream"}],"execution_count":31}]}